# âš™ï¸ Data Pipeline Automation

> Raw data is like crude oil â€” it only has value once you refine it.

An automated **ETL data pipeline** built with Apache Airflow, Python and PostgreSQL. Handles multi-source data ingestion, cleaning, transformation and loading â€” fully containerized with Docker Compose.

---

## ğŸ”„ Pipeline Overview

```
[Source A: CSV files]  â”€â”€â”
[Source B: REST API]   â”€â”€â”¼â”€â”€â–º EXTRACT â”€â”€â–º TRANSFORM â”€â”€â–º LOAD â”€â”€â–º PostgreSQL
[Source C: Database]   â”€â”€â”˜
                                               â”‚
                                        Airflow DAG
                                     (scheduled / triggered)
```

---

## ğŸ—‚ï¸ Project Structure

```
data-pipeline-automation/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pipeline_dag.py           # Airflow DAG definition
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py                # Data ingestion from multiple sources
â”‚   â”œâ”€â”€ transform.py              # Cleaning, normalization, feature engineering
â”‚   â””â”€â”€ load.py                   # Insert/upsert into PostgreSQL
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py               # DB credentials, paths, schedule config
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ create_tables.sql         # Schema initialization
â”‚
â”œâ”€â”€ docker-compose.yml            # Airflow + PostgreSQL + pgAdmin stack
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Quickstart

```bash
# Clone and start the full stack
docker-compose up -d

# Access Airflow UI
http://localhost:8080
# user: airflow / pass: airflow

# Access pgAdmin
http://localhost:5050
```

---

## ğŸ³ Services (Docker Compose)

| Service | Port | Description |
|---|---|---|
| Airflow Webserver | 8080 | DAG monitoring & triggering |
| PostgreSQL | 5432 | Data warehouse |
| pgAdmin | 5050 | DB management UI |

---

## ğŸ”§ ETL Steps

**Extract**
- CSV files from local or remote storage
- REST API calls with pagination handling
- SQL queries from source databases

**Transform**
- Null value handling & type casting
- Deduplication & outlier filtering
- Feature normalization & aggregation

**Load**
- Upsert into PostgreSQL target tables
- Load validation & row count checks
- Error logging & alerting

---

## â±ï¸ Scheduling

The DAG runs on a configurable cron schedule:

```python
schedule_interval="0 6 * * *"   # Every day at 6:00 AM
```

---

## ğŸ“¦ Tech Stack

- **Python 3.11**
- **Apache Airflow 2.9** â€” orchestration
- **Pandas** â€” data transformation
- **SQLAlchemy + psycopg2** â€” DB connection
- **PostgreSQL** â€” data storage
- **Docker / Docker Compose** â€” containerization

---

## ğŸ‘¤ Author

**ELOUAFI Abderrahmane**  
IngÃ©nieur Big Data & Cloud â€” ENSET Mohammedia  
[LinkedIn](https://www.linkedin.com/in/abderrahmane-elouafi-43226736b/) â€¢ [Portfolio](https://my-first-porfolio-six.vercel.app/)
